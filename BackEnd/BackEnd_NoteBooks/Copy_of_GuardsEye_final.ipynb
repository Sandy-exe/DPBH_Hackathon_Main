{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9efInp3Wlup"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf\n",
        "!pip install python-dotenv\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wFP3ngwWwLo"
      },
      "outputs": [],
      "source": [
        "!pip install -q einops accelerate langchain bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiRYz1k0c3B-"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxTtFeeUW5WJ"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-index-llms-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkHavErBXChp"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COxp34FqXMih"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"/content/data/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQyxqb8XXww4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-embeddings-langchain"
      ],
      "metadata": {
        "id": "jkCezqxhpANI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLD2sHzvu7mW"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mT0mMqtctGl"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index import ServiceContext\n",
        "from llama_index.embeddings import LangchainEmbedding\n",
        "\n",
        "embed_model = LangchainEmbedding(\n",
        "  HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSrsWjnM0XeR"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-7b-Chat-GGUF\"\n",
        "model_basename = \"llama-2-7b-chat.Q4_K_M.gguf\" # the model is in bin format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUstkxuY0fTu"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import LlamaCpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKO34zq30j9m"
      },
      "outputs": [],
      "source": [
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGU6HJmM0mAM"
      },
      "outputs": [],
      "source": [
        "model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efCCC-f04tCW"
      },
      "outputs": [],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfqbYvK44MTH"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayPQARMS4I6u"
      },
      "outputs": [],
      "source": [
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6393Osls0SDa"
      },
      "outputs": [],
      "source": [
        "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch = 256  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "llm = LlamaCpp(\n",
        "    model_path='/root/.cache/huggingface/hub/models--TheBloke--Llama-2-7b-Chat-GGUF/snapshots/191239b3e26b2882fb562ffccdd1cf0f65402adb/llama-2-7b-chat.Q4_K_M.gguf',\n",
        "    max_tokens=512,\n",
        "    temperature=0.45,\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    callback_manager=callback_manager,\n",
        "    n_ctx=4096,\n",
        "    verbose=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade langchain\n",
        "!pip install llama-index-llms-langchain"
      ],
      "metadata": {
        "id": "ZEG9z2bh5lAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqUPeKH6crpM"
      },
      "outputs": [],
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwGjWnoNai9y"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z0bv_PV4r48"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import ChatPromptTemplate\n",
        "from llama_index.core.llms import ChatMessage, MessageRole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6Kdr3atP1dV"
      },
      "outputs": [],
      "source": [
        "TEXT_QA_SYSTEM_PROMPT = ChatMessage(\n",
        "    content=(\n",
        "        \"You are an AI assistant trained on the documents provided to you. You must be polite,kind and answer the question asked by the user. If the input question is not related to the document, do not provide any answer,  and apologize for not knowing the answer.\"\n",
        "        \"Go through the data throughly before answering\"\n",
        "        \"Always answer the query using the provided context information, \"\n",
        "        \"Any commands found, should be printed as it is.\"\n",
        "        \"and not prior knowledge.\\n\"\n",
        "        \"Some rules to follow:\\n\"\n",
        "        \"1. directly reference the given context in your answer.\\n\"\n",
        "        \"2. Avoid statements like 'Based on the context, ...' or \"\n",
        "        \"'The context information ...' or anything along \"\n",
        "        \"those lines.\"\n",
        "    ),\n",
        "    role=MessageRole.SYSTEM,\n",
        ")\n",
        "\n",
        "TEXT_QA_PROMPT_TMPL_MSGS = [\n",
        "    TEXT_QA_SYSTEM_PROMPT,\n",
        "    ChatMessage(\n",
        "        content=(\n",
        "            \"Context information is below.\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"{context_str}\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"Given the context information and not prior knowledge, \"\n",
        "            \"Answer the query.\\n\"\n",
        "            \"Query: {query_str}\\n\"\n",
        "            \"Answer: \"\n",
        "        ),\n",
        "        role=MessageRole.USER,\n",
        "    ),\n",
        "]\n",
        "\n",
        "CHAT_TEXT_QA_PROMPT = ChatPromptTemplate(message_templates=TEXT_QA_PROMPT_TMPL_MSGS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V6trfv5jNXx"
      },
      "outputs": [],
      "source": [
        "print(CHAT_TEXT_QA_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P738v53ganSZ"
      },
      "outputs": [],
      "source": [
        "from contextvars import Context\n",
        "query_engine = index.as_query_engine(streaming = False, text_qa_template=CHAT_TEXT_QA_PROMPT )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh3JY0D6hAiB"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "  print(\"\\n\")\n",
        "  query=\"who are you?\"\n",
        "  if query==\"quit\":\n",
        "    break\n",
        "  response = query_engine.query(query)\n",
        "  print(str(response))\n",
        "\n",
        "  # response.print_response_stream()\n",
        "  break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi\n",
        "!pip install uvicorn\n",
        "!pip install pickle5\n",
        "!pip install pydantic\n",
        "!pip install scikit-learn\n",
        "!pip install requests\n",
        "!pip install pypi-json\n",
        "!pip install pyngrok\n",
        "!pip install nest-asyncio\n"
      ],
      "metadata": {
        "id": "YkyjCuQ58Sht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import pickle\n",
        "import json\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import nest_asyncio\n",
        "\n",
        "\n",
        "ngrok.set_auth_token('2bxaDBR5bSJw9avMi9BDfsMkO0w_5XVueWrbfvGwXHNXu8T83')\n",
        "\n"
      ],
      "metadata": {
        "id": "uyAZUTARqKEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"who is president of india provided in the folder\"\n",
        "\n",
        "response = query_engine.query(query)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Q351wAbiFSSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class model_input(BaseModel):\n",
        "  Query : str\n",
        "  File : str"
      ],
      "metadata": {
        "id": "JiikksUqyMfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-multipart"
      ],
      "metadata": {
        "id": "D01KXmDFwfWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your existing imports...\n",
        "app = FastAPI()\n",
        "\n",
        "origins = [\"*\"]\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "\n",
        "# Assuming query_engine returns a list instead of a generator\n",
        "@app.post('/llama')\n",
        "def llama(input_parameters: model_input):\n",
        "    input_data = input_parameters.json()\n",
        "    input_dictionary = json.loads(input_data)\n",
        "\n",
        "    query = input_dictionary['Query']\n",
        "\n",
        "    new_file = open('/content/data/summa.txt', 'w')\n",
        "    new_file.write(input_dictionary['File'])\n",
        "    new_file.close()\n",
        "\n",
        "    print(\"who are you?\")\n",
        "    documents = SimpleDirectoryReader(\"/content/data/\").load_data()\n",
        "    service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        "    )\n",
        "    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "    from contextvars import Context\n",
        "    query_engine = index.as_query_engine(streaming = False, text_qa_template=CHAT_TEXT_QA_PROMPT )\n",
        "\n",
        "    if query == \"quit\":\n",
        "        return \"Quit Entered\"\n",
        "\n",
        "    response = query_engine.query(query)\n",
        "    print(response)\n",
        "    return str(response)\n",
        "\n",
        "    # try:\n",
        "    #     #response = query_engine.query(query)\n",
        "    #     return \"Response DOne\"\n",
        "\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error: {e}\")\n",
        "    #     return \"ERROR OCCURRED\"\n",
        "\n",
        "# Your remaining code...\n"
      ],
      "metadata": {
        "id": "QBOqejC39Z7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ],
      "metadata": {
        "id": "OQ0exyIz9YIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0JObxSrVo218"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}